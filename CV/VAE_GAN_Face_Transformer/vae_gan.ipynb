{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c978c3e8",
   "metadata": {},
   "source": [
    "# VAE-GAN \n",
    "\n",
    "## Преобразование одного изображения в другое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "381bd449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import os\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd7a5cd",
   "metadata": {},
   "source": [
    "## Предобработка  датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cee4f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def crop_center_square(img: Image.Image) -> Image.Image:\n",
    "    width, height = img.size\n",
    "    side = min(width, height)\n",
    "    left = (width - side) // 2\n",
    "    top = (height - side) // 2\n",
    "    right = left + side\n",
    "    bottom = top + side\n",
    "    return img.crop((left, top, right, bottom))\n",
    "\n",
    "def process_dataset(input_dir, output_dir, size=128):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            with Image.open(input_path) as img:\n",
    "                img_cropped = crop_center_square(img)\n",
    "                img_resized = img_cropped.resize((size, size), Image.Resampling.LANCZOS)\n",
    "                if img_resized.mode == 'RGBA':\n",
    "                    img_resized = img_resized.convert('RGB')\n",
    "                img_resized.save(output_path)\n",
    "\n",
    "            print(f\"Processed {filename}\")\n",
    "\n",
    "# Пример использования\n",
    "input_folder = '/home/maksim/develops/python/develops_test/dataset_1/train/'\n",
    "output_folder = '/home/maksim/develops/python/develops_test/dataset_1/train_2/'\n",
    "\n",
    "# process_dataset(input_folder, output_folder, size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dcec9a",
   "metadata": {},
   "source": [
    "## Функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65a765f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Функции потерь -----\n",
    "\n",
    "# VAE loss: Reconstruction + KL divergence\n",
    "def vae_loss_function(recon_x, x, mu, logvar):\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')  # можно заменить на bce loss для бинарных данных\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + KLD, recon_loss, KLD\n",
    "\n",
    "# GAN потери (бинарная кросс-энтропия)\n",
    "# adversarial_loss = nn.BCELoss()\n",
    "adversarial_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "# def vae_loss_function(recon_x, x, mu, logvar):\n",
    "#     # Используем BCE loss для восстановления\n",
    "#     recon_loss = F.mse_loss(recon_x, x, reduction='mean')\n",
    "#     # KL дивергенция среднее по батчу\n",
    "#     KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "#     total_vae_loss = recon_loss + KLD\n",
    "#     return total_vae_loss, recon_loss, KLD\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36733683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_hinge_loss(real_out, fake_out):\n",
    "    loss_real = torch.mean(F.relu(1. - real_out))\n",
    "    loss_fake = torch.mean(F.relu(1. + fake_out))\n",
    "    return loss_real + loss_fake\n",
    "\n",
    "\n",
    "\n",
    "def generator_hinge_loss(fake_out):\n",
    "    return -torch.mean(fake_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ddb5d1",
   "metadata": {},
   "source": [
    "## Датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1558365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# ----- Класс датасета: загрузка изображений из папки -----\n",
    "class ImageFolderDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.filenames = [f for f in os.listdir(root_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.filenames[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014bfbee",
   "metadata": {},
   "source": [
    "## Функция тренировки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e9b7875",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- Ре параметризация -----\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps * std\n",
    "\n",
    "\n",
    "# ----- Функция тренировки одной эпохи -----\n",
    "def train_epoch(encoder, decoder, discriminator, dataloader, optim_enc_dec, optim_disc, device):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    total_vae_loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_kld_loss = 0\n",
    "    total_gan_disc_loss = 0\n",
    "    total_gan_gen_loss = 0\n",
    "\n",
    "    for batch_idx, imgs in enumerate(dataloader):\n",
    "        imgs = imgs.to(device)\n",
    "\n",
    "        batch_size = imgs.size(0)\n",
    "        # real_labels = torch.ones(batch_size, 1, device=device)\n",
    "        real_labels = torch.full((batch_size, 1), 0.9, device=device)\n",
    "        fake_labels = torch.zeros(batch_size, 1, device=device)\n",
    "\n",
    "        # === Обучение дискриминатора ===\n",
    "        optim_disc.zero_grad()\n",
    "        # Реальные изображения\n",
    "        real_out = discriminator(imgs)\n",
    "        loss_real = adversarial_loss(real_out, real_labels)\n",
    "        \n",
    "        # Сгенерированные модели изображения\n",
    "        mu, logvar = encoder(imgs)\n",
    "        z = reparameterize(mu, logvar)\n",
    "        fake_imgs = decoder(z)\n",
    "\n",
    "        fake_out = discriminator(fake_imgs.detach())\n",
    "        loss_fake = adversarial_loss(fake_out, fake_labels)\n",
    "\n",
    "        loss_disc = (loss_real + loss_fake) * 0.5\n",
    "        loss_disc.backward()\n",
    "        optim_disc.step()\n",
    "\n",
    "        # === Обучение энкодера и декодера (VAE + GAN) ===\n",
    "        optim_enc_dec.zero_grad()\n",
    "        \n",
    "        mu, logvar = encoder(imgs)\n",
    "        z = reparameterize(mu, logvar)\n",
    "        recon_imgs = decoder(z)\n",
    "\n",
    "        # VAE loss\n",
    "        vae_loss, recon_loss, kld_loss = vae_loss_function(recon_imgs, imgs, mu, logvar)\n",
    "\n",
    "\n",
    "\n",
    "        # GAN loss для генератора (декодера)\n",
    "        gen_out = discriminator(recon_imgs)\n",
    "        gan_gen_loss = adversarial_loss(gen_out, real_labels)  # хотим, чтобы сгенерированные проходили как \"реальные\"\n",
    "\n",
    "        #1e-4, 1e-3, 1e-2\n",
    "        alpha = 0.001\n",
    "        total_gen_loss = vae_loss +  alpha * gan_gen_loss  # балансируем веса\n",
    "\n",
    "        \n",
    "       \n",
    "\n",
    "        total_gen_loss.backward()\n",
    "        optim_enc_dec.step()\n",
    "\n",
    "        total_vae_loss += vae_loss.item()\n",
    "        total_recon_loss += recon_loss.item()\n",
    "        total_kld_loss += kld_loss.item()\n",
    "        total_gan_disc_loss += loss_disc.item()\n",
    "        total_gan_gen_loss += gan_gen_loss.item()\n",
    "\n",
    "    n = len(dataloader.dataset)\n",
    "\n",
    "    avg_vae_loss = total_vae_loss / n\n",
    "    avg_gan_disc_loss = total_gan_disc_loss / len(dataloader)\n",
    "    avg_gan_gen_loss = total_gan_gen_loss / len(dataloader)\n",
    "\n",
    "    print(f\"VAE Loss: {total_vae_loss/n:.4f}, Recon: {total_recon_loss/n:.4f}, KLD: {total_kld_loss/n:.4f}\")\n",
    "    print(f\"Discriminator Loss: {total_gan_disc_loss/len(dataloader):.4f}, Generator Loss (GAN): {total_gan_gen_loss/len(dataloader):.4f}\")\n",
    "\n",
    "    return avg_vae_loss, avg_gan_disc_loss, avg_gan_gen_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2263b03d",
   "metadata": {},
   "source": [
    "## Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "220f476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# ----- Архитектура VAE-GAN -----\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1),  # B,64, 64,64 for 128x128 inputs\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.fc_mu = nn.Linear(512*8*8, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(512*8*8, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.conv(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 512*8*8)\n",
    "\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),  # 256x16x16\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # 128x32x32\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),  # 64x64x64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1),  # 3x128x128\n",
    "            nn.Sigmoid()  # Чтобы на выходе были пиксели от 0 до 1  \n",
    "            # nn.Tanh()  #  Если нормализация\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        batch_size = z.size(0)\n",
    "        x = self.fc(z)\n",
    "        x = x.view(batch_size, 512, 8, 8)\n",
    "        x = self.deconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1),  # 64x64x64\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),  # 128x32x32\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),  # 256x16x16\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),  # 512x8x8\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512*8*8, 1),\n",
    "            # nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef43cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43865319",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1b6bd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "VAE Loss: 295.2565, Recon: 160.3926, KLD: 134.8639\n",
      "Discriminator Loss: 0.1673, Generator Loss (GAN): 11.1835\n",
      "Сохранена лучшая модель с VAE loss 295.2565\n",
      "Epoch 2/150\n",
      "VAE Loss: 295.0482, Recon: 160.1835, KLD: 134.8647\n",
      "Discriminator Loss: 0.1626, Generator Loss (GAN): 12.7851\n",
      "Сохранена лучшая модель с VAE loss 295.0482\n",
      "Epoch 3/150\n",
      "VAE Loss: 294.8541, Recon: 160.0889, KLD: 134.7651\n",
      "Discriminator Loss: 0.1626, Generator Loss (GAN): 12.9411\n",
      "Сохранена лучшая модель с VAE loss 294.8541\n",
      "Epoch 4/150\n",
      "VAE Loss: 295.0699, Recon: 160.2789, KLD: 134.7910\n",
      "Discriminator Loss: 0.1626, Generator Loss (GAN): 13.4160\n",
      "Epoch 5/150\n",
      "VAE Loss: 295.2850, Recon: 160.3585, KLD: 134.9265\n",
      "Discriminator Loss: 0.1626, Generator Loss (GAN): 13.2448\n",
      "Epoch 6/150\n",
      "VAE Loss: 295.2674, Recon: 160.4749, KLD: 134.7925\n",
      "Discriminator Loss: 0.1626, Generator Loss (GAN): 13.1520\n",
      "Epoch 7/150\n",
      "VAE Loss: 295.0436, Recon: 160.1593, KLD: 134.8842\n",
      "Discriminator Loss: 0.1626, Generator Loss (GAN): 13.0430\n",
      "Epoch 8/150\n",
      "VAE Loss: 294.5907, Recon: 159.8785, KLD: 134.7122\n",
      "Discriminator Loss: 0.1626, Generator Loss (GAN): 12.6254\n",
      "Сохранена лучшая модель с VAE loss 294.5907\n",
      "Epoch 9/150\n",
      "VAE Loss: 294.4778, Recon: 159.6976, KLD: 134.7803\n",
      "Discriminator Loss: 0.1626, Generator Loss (GAN): 13.3027\n",
      "Сохранена лучшая модель с VAE loss 294.4778\n",
      "Epoch 10/150\n",
      "VAE Loss: 294.3963, Recon: 159.7052, KLD: 134.6910\n",
      "Discriminator Loss: 0.1626, Generator Loss (GAN): 13.1638\n",
      "Сохранена лучшая модель с VAE loss 294.3963\n",
      "Epoch 11/150\n",
      "VAE Loss: 294.5416, Recon: 159.7818, KLD: 134.7598\n",
      "Discriminator Loss: 0.1626, Generator Loss (GAN): 13.0537\n",
      "Epoch 12/150\n",
      "VAE Loss: 294.8187, Recon: 159.9973, KLD: 134.8214\n",
      "Discriminator Loss: 0.1626, Generator Loss (GAN): 12.9108\n",
      "Epoch 13/150\n",
      "VAE Loss: 294.6042, Recon: 159.9276, KLD: 134.6765\n",
      "Discriminator Loss: 0.1626, Generator Loss (GAN): 13.2001\n",
      "Epoch 14/150\n",
      "VAE Loss: 294.4683, Recon: 159.7690, KLD: 134.6993\n",
      "Discriminator Loss: 0.4817, Generator Loss (GAN): 11.8068\n",
      "Epoch 15/150\n",
      "VAE Loss: 294.4414, Recon: 159.6736, KLD: 134.7678\n",
      "Discriminator Loss: 0.7226, Generator Loss (GAN): 0.8367\n",
      "Epoch 16/150\n",
      "VAE Loss: 294.0061, Recon: 159.4099, KLD: 134.5962\n",
      "Discriminator Loss: 0.5427, Generator Loss (GAN): 1.8718\n",
      "Сохранена лучшая модель с VAE loss 294.0061\n",
      "Epoch 17/150\n",
      "VAE Loss: 294.3057, Recon: 159.6224, KLD: 134.6833\n",
      "Discriminator Loss: 0.2092, Generator Loss (GAN): 5.1159\n",
      "Epoch 18/150\n",
      "VAE Loss: 294.2014, Recon: 159.5005, KLD: 134.7009\n",
      "Discriminator Loss: 0.1728, Generator Loss (GAN): 7.1675\n",
      "Epoch 19/150\n",
      "VAE Loss: 294.1050, Recon: 159.4510, KLD: 134.6540\n",
      "Discriminator Loss: 0.1667, Generator Loss (GAN): 8.7482\n",
      "Epoch 20/150\n",
      "VAE Loss: 293.9149, Recon: 159.2625, KLD: 134.6524\n",
      "Discriminator Loss: 0.1649, Generator Loss (GAN): 9.2822\n",
      "Сохранена лучшая модель с VAE loss 293.9149\n",
      "Epoch 21/150\n",
      "VAE Loss: 294.0094, Recon: 159.4453, KLD: 134.5641\n",
      "Discriminator Loss: 0.1666, Generator Loss (GAN): 9.5058\n",
      "Epoch 22/150\n",
      "VAE Loss: 294.1354, Recon: 159.4131, KLD: 134.7223\n",
      "Discriminator Loss: 0.1642, Generator Loss (GAN): 9.6183\n",
      "Epoch 23/150\n",
      "VAE Loss: 294.0330, Recon: 159.3660, KLD: 134.6670\n",
      "Discriminator Loss: 0.1635, Generator Loss (GAN): 10.5421\n",
      "Epoch 24/150\n",
      "VAE Loss: 294.3612, Recon: 159.6478, KLD: 134.7133\n",
      "Discriminator Loss: 0.2206, Generator Loss (GAN): 9.5240\n",
      "Epoch 25/150\n",
      "VAE Loss: 293.8616, Recon: 159.1067, KLD: 134.7548\n",
      "Discriminator Loss: 0.6497, Generator Loss (GAN): 1.3058\n",
      "Сохранена лучшая модель с VAE loss 293.8616\n",
      "Epoch 26/150\n",
      "VAE Loss: 293.5509, Recon: 158.9975, KLD: 134.5533\n",
      "Discriminator Loss: 0.2370, Generator Loss (GAN): 5.5553\n",
      "Сохранена лучшая модель с VAE loss 293.5509\n",
      "Epoch 27/150\n",
      "VAE Loss: 293.7798, Recon: 159.1123, KLD: 134.6675\n",
      "Discriminator Loss: 0.1705, Generator Loss (GAN): 7.1132\n",
      "Epoch 28/150\n",
      "VAE Loss: 293.9266, Recon: 159.3036, KLD: 134.6231\n",
      "Discriminator Loss: 0.1649, Generator Loss (GAN): 8.4725\n",
      "Epoch 29/150\n",
      "VAE Loss: 293.5525, Recon: 158.8868, KLD: 134.6657\n",
      "Discriminator Loss: 0.1637, Generator Loss (GAN): 9.1859\n",
      "Epoch 30/150\n",
      "VAE Loss: 293.4610, Recon: 158.8823, KLD: 134.5787\n",
      "Discriminator Loss: 0.1633, Generator Loss (GAN): 9.3566\n",
      "Сохранена лучшая модель с VAE loss 293.4610\n",
      "Epoch 31/150\n",
      "VAE Loss: 293.7330, Recon: 159.0859, KLD: 134.6471\n",
      "Discriminator Loss: 0.1632, Generator Loss (GAN): 9.3572\n",
      "Epoch 32/150\n",
      "VAE Loss: 293.6518, Recon: 158.9847, KLD: 134.6671\n",
      "Discriminator Loss: 0.1632, Generator Loss (GAN): 9.4262\n",
      "Epoch 33/150\n",
      "VAE Loss: 293.5154, Recon: 159.0172, KLD: 134.4982\n",
      "Discriminator Loss: 0.1629, Generator Loss (GAN): 9.9870\n",
      "Epoch 34/150\n",
      "VAE Loss: 293.5567, Recon: 159.0406, KLD: 134.5161\n",
      "Discriminator Loss: 0.1631, Generator Loss (GAN): 10.1553\n",
      "Epoch 35/150\n",
      "VAE Loss: 293.2983, Recon: 158.8670, KLD: 134.4314\n",
      "Discriminator Loss: 0.1629, Generator Loss (GAN): 9.9821\n",
      "Сохранена лучшая модель с VAE loss 293.2983\n",
      "Epoch 36/150\n",
      "VAE Loss: 293.1847, Recon: 158.7067, KLD: 134.4780\n",
      "Discriminator Loss: 0.1629, Generator Loss (GAN): 10.0890\n",
      "Сохранена лучшая модель с VAE loss 293.1847\n",
      "Epoch 37/150\n",
      "VAE Loss: 293.0972, Recon: 158.4588, KLD: 134.6384\n",
      "Discriminator Loss: 0.1629, Generator Loss (GAN): 10.1391\n",
      "Сохранена лучшая модель с VAE loss 293.0972\n",
      "Epoch 38/150\n",
      "VAE Loss: 293.0449, Recon: 158.5934, KLD: 134.4515\n",
      "Discriminator Loss: 0.1632, Generator Loss (GAN): 10.0715\n",
      "Сохранена лучшая модель с VAE loss 293.0449\n",
      "Epoch 39/150\n",
      "VAE Loss: 293.2430, Recon: 158.6162, KLD: 134.6269\n",
      "Discriminator Loss: 0.1628, Generator Loss (GAN): 10.4001\n",
      "Epoch 40/150\n",
      "VAE Loss: 293.0908, Recon: 158.4435, KLD: 134.6473\n",
      "Discriminator Loss: 0.1629, Generator Loss (GAN): 10.2439\n",
      "Epoch 41/150\n",
      "VAE Loss: 293.1026, Recon: 158.6125, KLD: 134.4902\n",
      "Discriminator Loss: 0.1631, Generator Loss (GAN): 10.2772\n",
      "Epoch 42/150\n",
      "VAE Loss: 293.1325, Recon: 158.5857, KLD: 134.5468\n",
      "Discriminator Loss: 0.1628, Generator Loss (GAN): 10.1679\n",
      "Epoch 43/150\n",
      "VAE Loss: 293.0719, Recon: 158.6508, KLD: 134.4210\n",
      "Discriminator Loss: 0.1627, Generator Loss (GAN): 10.3713\n",
      "Epoch 44/150\n",
      "VAE Loss: 292.6972, Recon: 158.2815, KLD: 134.4157\n",
      "Discriminator Loss: 0.1628, Generator Loss (GAN): 10.3643\n",
      "Сохранена лучшая модель с VAE loss 292.6972\n",
      "Epoch 45/150\n",
      "VAE Loss: 292.7947, Recon: 158.3756, KLD: 134.4191\n",
      "Discriminator Loss: 0.1627, Generator Loss (GAN): 10.2282\n",
      "Epoch 46/150\n",
      "VAE Loss: 292.5066, Recon: 158.1854, KLD: 134.3212\n",
      "Discriminator Loss: 0.5715, Generator Loss (GAN): 6.7403\n",
      "Сохранена лучшая модель с VAE loss 292.5066\n",
      "Epoch 47/150\n",
      "VAE Loss: 292.4727, Recon: 158.1298, KLD: 134.3429\n",
      "Discriminator Loss: 0.6953, Generator Loss (GAN): 0.9236\n",
      "Сохранена лучшая модель с VAE loss 292.4727\n",
      "Epoch 48/150\n",
      "VAE Loss: 292.5876, Recon: 158.2319, KLD: 134.3558\n",
      "Discriminator Loss: 0.2680, Generator Loss (GAN): 4.7482\n",
      "Epoch 49/150\n",
      "VAE Loss: 292.7810, Recon: 158.2461, KLD: 134.5349\n",
      "Discriminator Loss: 0.1692, Generator Loss (GAN): 6.7589\n",
      "Epoch 50/150\n",
      "VAE Loss: 292.6114, Recon: 158.1970, KLD: 134.4145\n",
      "Discriminator Loss: 0.1660, Generator Loss (GAN): 7.7755\n",
      "Epoch 51/150\n",
      "VAE Loss: 292.9987, Recon: 158.4513, KLD: 134.5475\n",
      "Discriminator Loss: 0.1657, Generator Loss (GAN): 8.0110\n",
      "Epoch 52/150\n",
      "VAE Loss: 292.5521, Recon: 158.0670, KLD: 134.4850\n",
      "Discriminator Loss: 0.1706, Generator Loss (GAN): 8.1597\n",
      "Epoch 53/150\n",
      "VAE Loss: 292.4140, Recon: 158.0188, KLD: 134.3952\n",
      "Discriminator Loss: 0.1650, Generator Loss (GAN): 7.8780\n",
      "Сохранена лучшая модель с VAE loss 292.4140\n",
      "Epoch 54/150\n",
      "VAE Loss: 291.9776, Recon: 157.7101, KLD: 134.2675\n",
      "Discriminator Loss: 0.1639, Generator Loss (GAN): 8.2481\n",
      "Сохранена лучшая модель с VAE loss 291.9776\n",
      "Epoch 55/150\n",
      "VAE Loss: 292.2366, Recon: 157.9204, KLD: 134.3163\n",
      "Discriminator Loss: 0.1636, Generator Loss (GAN): 8.5579\n",
      "Epoch 56/150\n",
      "VAE Loss: 292.2503, Recon: 157.8909, KLD: 134.3593\n",
      "Discriminator Loss: 0.1631, Generator Loss (GAN): 9.1208\n",
      "Epoch 57/150\n",
      "VAE Loss: 292.3638, Recon: 158.1378, KLD: 134.2260\n",
      "Discriminator Loss: 0.1628, Generator Loss (GAN): 9.6477\n",
      "Epoch 58/150\n",
      "VAE Loss: 292.1638, Recon: 157.9786, KLD: 134.1852\n",
      "Discriminator Loss: 0.1627, Generator Loss (GAN): 9.9040\n",
      "Epoch 59/150\n",
      "VAE Loss: 291.8313, Recon: 157.6078, KLD: 134.2235\n",
      "Discriminator Loss: 0.2970, Generator Loss (GAN): 8.3636\n",
      "Сохранена лучшая модель с VAE loss 291.8313\n",
      "Epoch 60/150\n",
      "VAE Loss: 291.9387, Recon: 157.7035, KLD: 134.2353\n",
      "Discriminator Loss: 0.1648, Generator Loss (GAN): 8.5558\n",
      "Epoch 61/150\n",
      "VAE Loss: 292.0058, Recon: 157.6555, KLD: 134.3503\n",
      "Discriminator Loss: 0.1641, Generator Loss (GAN): 8.8560\n",
      "Epoch 62/150\n",
      "VAE Loss: 292.0310, Recon: 157.8334, KLD: 134.1977\n",
      "Discriminator Loss: 0.1633, Generator Loss (GAN): 9.1460\n",
      "Epoch 63/150\n",
      "VAE Loss: 291.9127, Recon: 157.6841, KLD: 134.2286\n",
      "Discriminator Loss: 0.1630, Generator Loss (GAN): 9.5258\n",
      "Epoch 64/150\n",
      "VAE Loss: 291.5735, Recon: 157.4975, KLD: 134.0760\n",
      "Discriminator Loss: 0.1629, Generator Loss (GAN): 9.9392\n",
      "Сохранена лучшая модель с VAE loss 291.5735\n",
      "Epoch 65/150\n",
      "VAE Loss: 291.8918, Recon: 157.6893, KLD: 134.2025\n",
      "Discriminator Loss: 0.1630, Generator Loss (GAN): 10.7288\n",
      "Epoch 66/150\n",
      "VAE Loss: 291.7121, Recon: 157.4785, KLD: 134.2336\n",
      "Discriminator Loss: 0.1628, Generator Loss (GAN): 10.5674\n",
      "Epoch 67/150\n",
      "VAE Loss: 291.5786, Recon: 157.3801, KLD: 134.1986\n",
      "Discriminator Loss: 0.1628, Generator Loss (GAN): 10.8777\n",
      "Epoch 68/150\n",
      "VAE Loss: 291.6088, Recon: 157.3236, KLD: 134.2852\n",
      "Discriminator Loss: 0.1629, Generator Loss (GAN): 11.0228\n",
      "Epoch 69/150\n",
      "VAE Loss: 291.7235, Recon: 157.4888, KLD: 134.2348\n",
      "Discriminator Loss: 0.1627, Generator Loss (GAN): 11.2855\n",
      "Epoch 70/150\n",
      "VAE Loss: 291.5621, Recon: 157.3369, KLD: 134.2252\n",
      "Discriminator Loss: 0.1627, Generator Loss (GAN): 11.3439\n",
      "Сохранена лучшая модель с VAE loss 291.5621\n",
      "Epoch 71/150\n",
      "VAE Loss: 291.5773, Recon: 157.4237, KLD: 134.1535\n",
      "Discriminator Loss: 0.1627, Generator Loss (GAN): 11.5590\n",
      "Epoch 72/150\n",
      "VAE Loss: 291.6272, Recon: 157.5358, KLD: 134.0914\n",
      "Discriminator Loss: 0.1626, Generator Loss (GAN): 11.9794\n",
      "Epoch 73/150\n",
      "VAE Loss: 291.4033, Recon: 157.3604, KLD: 134.0429\n",
      "Discriminator Loss: 0.1627, Generator Loss (GAN): 11.8442\n",
      "Сохранена лучшая модель с VAE loss 291.4033\n",
      "Epoch 74/150\n",
      "VAE Loss: 291.2806, Recon: 157.0940, KLD: 134.1866\n",
      "Discriminator Loss: 0.1627, Generator Loss (GAN): 11.8939\n",
      "Сохранена лучшая модель с VAE loss 291.2806\n",
      "Epoch 75/150\n",
      "VAE Loss: 291.3173, Recon: 157.2696, KLD: 134.0478\n",
      "Discriminator Loss: 0.1626, Generator Loss (GAN): 11.8193\n",
      "Epoch 76/150\n",
      "VAE Loss: 291.3071, Recon: 157.1613, KLD: 134.1459\n",
      "Discriminator Loss: 0.1627, Generator Loss (GAN): 11.9289\n",
      "Epoch 77/150\n",
      "VAE Loss: 291.0840, Recon: 156.9571, KLD: 134.1269\n",
      "Discriminator Loss: 0.1627, Generator Loss (GAN): 12.0346\n",
      "Сохранена лучшая модель с VAE loss 291.0840\n",
      "Epoch 78/150\n",
      "VAE Loss: 291.2666, Recon: 157.1894, KLD: 134.0773\n",
      "Discriminator Loss: 0.1626, Generator Loss (GAN): 12.0503\n",
      "Epoch 79/150\n",
      "VAE Loss: 290.9170, Recon: 156.9338, KLD: 133.9833\n",
      "Discriminator Loss: 0.1627, Generator Loss (GAN): 12.4038\n",
      "Сохранена лучшая модель с VAE loss 290.9170\n",
      "Epoch 80/150\n",
      "VAE Loss: 291.0662, Recon: 157.0600, KLD: 134.0062\n",
      "Discriminator Loss: 0.1626, Generator Loss (GAN): 12.0648\n",
      "Epoch 81/150\n",
      "VAE Loss: 291.0995, Recon: 157.0114, KLD: 134.0880\n",
      "Discriminator Loss: 0.4027, Generator Loss (GAN): 10.6176\n",
      "Epoch 82/150\n",
      "VAE Loss: 290.8727, Recon: 156.7461, KLD: 134.1266\n",
      "Discriminator Loss: 0.1779, Generator Loss (GAN): 6.9271\n",
      "Сохранена лучшая модель с VAE loss 290.8727\n",
      "Epoch 83/150\n",
      "VAE Loss: 290.9943, Recon: 156.9398, KLD: 134.0545\n",
      "Discriminator Loss: 0.1655, Generator Loss (GAN): 8.1453\n",
      "Epoch 84/150\n",
      "VAE Loss: 290.7808, Recon: 156.7443, KLD: 134.0365\n",
      "Discriminator Loss: 0.1652, Generator Loss (GAN): 8.7032\n",
      "Сохранена лучшая модель с VAE loss 290.7808\n",
      "Epoch 85/150\n",
      "VAE Loss: 290.7255, Recon: 156.7298, KLD: 133.9957\n",
      "Discriminator Loss: 0.1638, Generator Loss (GAN): 9.3651\n",
      "Сохранена лучшая модель с VAE loss 290.7255\n",
      "Epoch 86/150\n",
      "VAE Loss: 290.9095, Recon: 156.8928, KLD: 134.0167\n",
      "Discriminator Loss: 0.1633, Generator Loss (GAN): 9.9695\n",
      "Epoch 87/150\n",
      "VAE Loss: 290.5833, Recon: 156.7346, KLD: 133.8487\n",
      "Discriminator Loss: 0.1631, Generator Loss (GAN): 10.5474\n",
      "Сохранена лучшая модель с VAE loss 290.5833\n",
      "Epoch 88/150\n",
      "VAE Loss: 290.6489, Recon: 156.6707, KLD: 133.9782\n",
      "Discriminator Loss: 0.1630, Generator Loss (GAN): 11.0929\n",
      "Epoch 89/150\n",
      "VAE Loss: 290.6630, Recon: 156.6818, KLD: 133.9811\n",
      "Discriminator Loss: 0.1630, Generator Loss (GAN): 11.8210\n",
      "Epoch 90/150\n",
      "VAE Loss: 290.5860, Recon: 156.5685, KLD: 134.0175\n",
      "Discriminator Loss: 0.1629, Generator Loss (GAN): 11.4287\n",
      "Epoch 91/150\n",
      "VAE Loss: 290.4144, Recon: 156.5282, KLD: 133.8862\n",
      "Discriminator Loss: 0.1628, Generator Loss (GAN): 11.5290\n",
      "Сохранена лучшая модель с VAE loss 290.4144\n",
      "Epoch 92/150\n",
      "VAE Loss: 290.6540, Recon: 156.6437, KLD: 134.0103\n",
      "Discriminator Loss: 0.1629, Generator Loss (GAN): 11.5937\n",
      "Epoch 93/150\n",
      "VAE Loss: 290.4245, Recon: 156.5258, KLD: 133.8986\n",
      "Discriminator Loss: 0.1628, Generator Loss (GAN): 12.0820\n",
      "Epoch 94/150\n",
      "VAE Loss: 290.8144, Recon: 156.7808, KLD: 134.0336\n",
      "Discriminator Loss: 0.1631, Generator Loss (GAN): 12.1183\n",
      "Epoch 95/150\n",
      "VAE Loss: 290.4029, Recon: 156.4982, KLD: 133.9047\n",
      "Discriminator Loss: 0.1630, Generator Loss (GAN): 11.9602\n",
      "Сохранена лучшая модель с VAE loss 290.4029\n",
      "Epoch 96/150\n",
      "VAE Loss: 290.3107, Recon: 156.3892, KLD: 133.9215\n",
      "Discriminator Loss: 0.1628, Generator Loss (GAN): 11.9751\n",
      "Сохранена лучшая модель с VAE loss 290.3107\n",
      "Epoch 97/150\n",
      "VAE Loss: 290.1663, Recon: 156.2096, KLD: 133.9567\n",
      "Discriminator Loss: 0.1627, Generator Loss (GAN): 12.0004\n",
      "Сохранена лучшая модель с VAE loss 290.1663\n",
      "Epoch 98/150\n",
      "VAE Loss: 290.0572, Recon: 156.0987, KLD: 133.9585\n",
      "Discriminator Loss: 0.1633, Generator Loss (GAN): 11.8626\n",
      "Сохранена лучшая модель с VAE loss 290.0572\n",
      "Epoch 99/150\n",
      "VAE Loss: 290.1465, Recon: 156.3516, KLD: 133.7949\n",
      "Discriminator Loss: 0.3187, Generator Loss (GAN): 13.6453\n",
      "Epoch 100/150\n",
      "VAE Loss: 289.9325, Recon: 156.2203, KLD: 133.7121\n",
      "Discriminator Loss: 0.3069, Generator Loss (GAN): 14.4702\n",
      "Сохранена лучшая модель с VAE loss 289.9325\n",
      "Epoch 101/150\n",
      "VAE Loss: 289.9727, Recon: 156.0679, KLD: 133.9049\n",
      "Discriminator Loss: 0.1663, Generator Loss (GAN): 11.3400\n",
      "Epoch 102/150\n",
      "VAE Loss: 289.9858, Recon: 156.0750, KLD: 133.9107\n",
      "Discriminator Loss: 0.1677, Generator Loss (GAN): 10.7244\n",
      "Epoch 103/150\n",
      "VAE Loss: 290.0253, Recon: 156.2415, KLD: 133.7838\n",
      "Discriminator Loss: 0.1657, Generator Loss (GAN): 10.7882\n",
      "Epoch 104/150\n",
      "VAE Loss: 290.0769, Recon: 156.2450, KLD: 133.8320\n",
      "Discriminator Loss: 0.1641, Generator Loss (GAN): 11.5263\n",
      "Epoch 105/150\n",
      "VAE Loss: 290.0218, Recon: 156.0665, KLD: 133.9553\n",
      "Discriminator Loss: 0.1632, Generator Loss (GAN): 11.4474\n",
      "Epoch 106/150\n",
      "VAE Loss: 289.7133, Recon: 155.9698, KLD: 133.7435\n",
      "Discriminator Loss: 0.1631, Generator Loss (GAN): 11.5806\n",
      "Сохранена лучшая модель с VAE loss 289.7133\n",
      "Epoch 107/150\n",
      "VAE Loss: 289.7543, Recon: 155.8863, KLD: 133.8680\n",
      "Discriminator Loss: 0.1629, Generator Loss (GAN): 11.9837\n",
      "Epoch 108/150\n",
      "VAE Loss: 289.7829, Recon: 156.0723, KLD: 133.7106\n",
      "Discriminator Loss: 0.1633, Generator Loss (GAN): 11.8656\n",
      "Epoch 109/150\n",
      "VAE Loss: 289.7811, Recon: 155.9947, KLD: 133.7864\n",
      "Discriminator Loss: 0.1630, Generator Loss (GAN): 12.0001\n",
      "Epoch 110/150\n",
      "VAE Loss: 289.7778, Recon: 156.0295, KLD: 133.7483\n",
      "Discriminator Loss: 0.1629, Generator Loss (GAN): 12.0270\n",
      "Epoch 111/150\n",
      "VAE Loss: 289.5781, Recon: 155.8845, KLD: 133.6936\n",
      "Discriminator Loss: 0.1629, Generator Loss (GAN): 12.5444\n",
      "Сохранена лучшая модель с VAE loss 289.5781\n",
      "Epoch 112/150\n",
      "VAE Loss: 289.5933, Recon: 155.7849, KLD: 133.8084\n",
      "Discriminator Loss: 0.1630, Generator Loss (GAN): 12.2121\n",
      "Epoch 113/150\n",
      "VAE Loss: 289.3840, Recon: 155.7539, KLD: 133.6301\n",
      "Discriminator Loss: 0.1630, Generator Loss (GAN): 12.1290\n",
      "Сохранена лучшая модель с VAE loss 289.3840\n",
      "Epoch 114/150\n",
      "VAE Loss: 289.6915, Recon: 155.8867, KLD: 133.8047\n",
      "Discriminator Loss: 0.1636, Generator Loss (GAN): 12.0691\n",
      "Epoch 115/150\n",
      "VAE Loss: 289.5251, Recon: 155.6864, KLD: 133.8387\n",
      "Discriminator Loss: 0.1628, Generator Loss (GAN): 12.3193\n",
      "Epoch 116/150\n",
      "VAE Loss: 289.5462, Recon: 155.7397, KLD: 133.8065\n",
      "Discriminator Loss: 0.1629, Generator Loss (GAN): 12.3007\n",
      "Epoch 117/150\n",
      "VAE Loss: 289.2903, Recon: 155.6257, KLD: 133.6646\n",
      "Discriminator Loss: 0.1630, Generator Loss (GAN): 12.6245\n",
      "Сохранена лучшая модель с VAE loss 289.2903\n",
      "Epoch 118/150\n",
      "VAE Loss: 289.4840, Recon: 155.8085, KLD: 133.6755\n",
      "Discriminator Loss: 0.1628, Generator Loss (GAN): 12.7225\n",
      "Epoch 119/150\n",
      "VAE Loss: 289.0769, Recon: 155.4209, KLD: 133.6560\n",
      "Discriminator Loss: 0.1628, Generator Loss (GAN): 12.1321\n",
      "Сохранена лучшая модель с VAE loss 289.0769\n",
      "Epoch 120/150\n",
      "VAE Loss: 289.1055, Recon: 155.4359, KLD: 133.6696\n",
      "Discriminator Loss: 0.1628, Generator Loss (GAN): 12.2060\n",
      "Epoch 121/150\n",
      "VAE Loss: 288.9810, Recon: 155.2829, KLD: 133.6981\n",
      "Discriminator Loss: 0.1630, Generator Loss (GAN): 12.1157\n",
      "Сохранена лучшая модель с VAE loss 288.9810\n",
      "Epoch 122/150\n",
      "VAE Loss: 288.9730, Recon: 155.3345, KLD: 133.6384\n",
      "Discriminator Loss: 0.1633, Generator Loss (GAN): 11.3482\n",
      "Сохранена лучшая модель с VAE loss 288.9730\n",
      "Epoch 123/150\n",
      "VAE Loss: 289.3159, Recon: 155.5137, KLD: 133.8022\n",
      "Discriminator Loss: 0.1628, Generator Loss (GAN): 11.6263\n",
      "Epoch 124/150\n",
      "VAE Loss: 289.0043, Recon: 155.2709, KLD: 133.7334\n",
      "Discriminator Loss: 0.5457, Generator Loss (GAN): 20.2051\n",
      "Epoch 125/150\n",
      "VAE Loss: 288.9876, Recon: 155.4546, KLD: 133.5331\n",
      "Discriminator Loss: 0.1778, Generator Loss (GAN): 25.4874\n",
      "Epoch 126/150\n",
      "VAE Loss: 288.8973, Recon: 155.2554, KLD: 133.6419\n",
      "Discriminator Loss: 0.1677, Generator Loss (GAN): 21.3089\n",
      "Сохранена лучшая модель с VAE loss 288.8973\n",
      "Epoch 127/150\n",
      "VAE Loss: 288.8408, Recon: 155.2572, KLD: 133.5836\n",
      "Discriminator Loss: 0.1654, Generator Loss (GAN): 20.2936\n",
      "Сохранена лучшая модель с VAE loss 288.8408\n",
      "Epoch 128/150\n",
      "VAE Loss: 289.0003, Recon: 155.4875, KLD: 133.5128\n",
      "Discriminator Loss: 0.1641, Generator Loss (GAN): 19.7609\n",
      "Epoch 129/150\n",
      "VAE Loss: 289.0884, Recon: 155.3531, KLD: 133.7353\n",
      "Discriminator Loss: 0.1639, Generator Loss (GAN): 20.0941\n",
      "Epoch 130/150\n",
      "VAE Loss: 288.7355, Recon: 155.1605, KLD: 133.5750\n",
      "Discriminator Loss: 0.1636, Generator Loss (GAN): 19.7356\n",
      "Сохранена лучшая модель с VAE loss 288.7355\n",
      "Epoch 131/150\n",
      "VAE Loss: 288.6777, Recon: 154.9923, KLD: 133.6855\n",
      "Discriminator Loss: 0.1635, Generator Loss (GAN): 19.6679\n",
      "Сохранена лучшая модель с VAE loss 288.6777\n",
      "Epoch 132/150\n",
      "VAE Loss: 288.6581, Recon: 155.2092, KLD: 133.4489\n",
      "Discriminator Loss: 0.1632, Generator Loss (GAN): 18.9207\n",
      "Сохранена лучшая модель с VAE loss 288.6581\n",
      "Epoch 133/150\n",
      "VAE Loss: 288.4937, Recon: 154.9602, KLD: 133.5335\n",
      "Discriminator Loss: 0.1631, Generator Loss (GAN): 18.6264\n",
      "Сохранена лучшая модель с VAE loss 288.4937\n",
      "Epoch 134/150\n",
      "VAE Loss: 288.4057, Recon: 154.9929, KLD: 133.4127\n",
      "Discriminator Loss: 0.1634, Generator Loss (GAN): 19.4412\n",
      "Сохранена лучшая модель с VAE loss 288.4057\n",
      "Epoch 135/150\n",
      "VAE Loss: 288.7699, Recon: 155.2009, KLD: 133.5690\n",
      "Discriminator Loss: 0.1633, Generator Loss (GAN): 19.5786\n",
      "Epoch 136/150\n",
      "VAE Loss: 288.5194, Recon: 155.0665, KLD: 133.4529\n",
      "Discriminator Loss: 0.1637, Generator Loss (GAN): 18.9895\n",
      "Epoch 137/150\n",
      "VAE Loss: 288.2499, Recon: 154.8520, KLD: 133.3980\n",
      "Discriminator Loss: 0.1633, Generator Loss (GAN): 19.3992\n",
      "Сохранена лучшая модель с VAE loss 288.2499\n",
      "Epoch 138/150\n",
      "VAE Loss: 288.4371, Recon: 154.9274, KLD: 133.5097\n",
      "Discriminator Loss: 0.1633, Generator Loss (GAN): 19.0142\n",
      "Epoch 139/150\n",
      "VAE Loss: 288.3143, Recon: 154.8765, KLD: 133.4378\n",
      "Discriminator Loss: 0.1635, Generator Loss (GAN): 18.6373\n",
      "Epoch 140/150\n",
      "VAE Loss: 288.6075, Recon: 155.0212, KLD: 133.5863\n",
      "Discriminator Loss: 0.1636, Generator Loss (GAN): 19.7036\n",
      "Epoch 141/150\n",
      "VAE Loss: 288.3724, Recon: 154.8823, KLD: 133.4901\n",
      "Discriminator Loss: 0.1635, Generator Loss (GAN): 18.4974\n",
      "Epoch 142/150\n",
      "VAE Loss: 288.3813, Recon: 155.0297, KLD: 133.3516\n",
      "Discriminator Loss: 0.2163, Generator Loss (GAN): 18.3800\n",
      "Epoch 143/150\n",
      "VAE Loss: 288.2338, Recon: 154.8078, KLD: 133.4260\n",
      "Discriminator Loss: 0.1711, Generator Loss (GAN): 17.0575\n",
      "Сохранена лучшая модель с VAE loss 288.2338\n",
      "Epoch 144/150\n",
      "VAE Loss: 288.5137, Recon: 154.9853, KLD: 133.5284\n",
      "Discriminator Loss: 0.1642, Generator Loss (GAN): 15.5589\n",
      "Epoch 145/150\n",
      "VAE Loss: 288.0415, Recon: 154.5867, KLD: 133.4548\n",
      "Discriminator Loss: 0.1636, Generator Loss (GAN): 16.2547\n",
      "Сохранена лучшая модель с VAE loss 288.0415\n",
      "Epoch 146/150\n",
      "VAE Loss: 288.0264, Recon: 154.5334, KLD: 133.4929\n",
      "Discriminator Loss: 0.1632, Generator Loss (GAN): 15.1645\n",
      "Сохранена лучшая модель с VAE loss 288.0264\n",
      "Epoch 147/150\n",
      "VAE Loss: 288.4095, Recon: 154.8968, KLD: 133.5127\n",
      "Discriminator Loss: 0.1666, Generator Loss (GAN): 16.6451\n",
      "Epoch 148/150\n",
      "VAE Loss: 288.2546, Recon: 154.8246, KLD: 133.4300\n",
      "Discriminator Loss: 0.1633, Generator Loss (GAN): 15.5362\n",
      "Epoch 149/150\n",
      "VAE Loss: 288.1639, Recon: 154.7965, KLD: 133.3674\n",
      "Discriminator Loss: 0.1630, Generator Loss (GAN): 15.5022\n",
      "Epoch 150/150\n",
      "VAE Loss: 287.8801, Recon: 154.4752, KLD: 133.4049\n",
      "Discriminator Loss: 0.1630, Generator Loss (GAN): 15.8779\n",
      "Сохранена лучшая модель с VAE loss 287.8801\n"
     ]
    }
   ],
   "source": [
    "# ----- Пример запуска -----\n",
    "if __name__ == \"__main__\":\n",
    "    from torchvision.datasets import ImageFolder\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Параметры модели\n",
    "    latent_dim = 128\n",
    "    batch_size = 64\n",
    "    image_size = 128  # предполагается квадратное изображение 128x128\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "    # transform = transforms.Compose([\n",
    "    #     transforms.Resize((image_size, image_size)),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Normalize(mean=mean,  # средние значения каналов ImageNet\n",
    "    #                         std=std)   # стандартные отклонения каналов ImageNet\n",
    "    # ])\n",
    "\n",
    "\n",
    "    inv_normalize = T.Normalize(\n",
    "        mean=[-m/s for m, s in zip(mean, std)],\n",
    "        std=[1/s for s in std]\n",
    "    )\n",
    "\n",
    "    # dataset = ImageFolderDataset('/home/maksim/develops/python/develops_test/dataset/face/3/train/', transform=transform)\n",
    "    dataset = ImageFolderDataset('/home/maksim/develops/python/develops_test/dataset_1/train_2/', transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    encoder = Encoder(latent_dim).to(device)\n",
    "    decoder = Decoder(latent_dim).to(device)\n",
    "    discriminator = Discriminator().to(device)\n",
    "\n",
    "    encoder.load_state_dict(torch.load('best_encoder.pth'))\n",
    "    decoder.load_state_dict(torch.load('best_decoder.pth'))\n",
    "    discriminator.load_state_dict(torch.load('best_discriminator.pth'))\n",
    "\n",
    "\n",
    "    optim_enc_dec = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-4)\n",
    "    optim_disc = torch.optim.Adam(discriminator.parameters(), lr=1e-2)\n",
    "\n",
    "    num_epochs = 150\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        avg_vae_loss, avg_gan_disc_loss, avg_gan_gen_loss = train_epoch(encoder, decoder, discriminator, dataloader, optim_enc_dec, optim_disc, device)\n",
    "\n",
    "        if avg_vae_loss < best_loss:\n",
    "            best_loss = avg_vae_loss\n",
    "            \n",
    "            torch.save(encoder.state_dict(), 'best_encoder.pth')\n",
    "            torch.save(decoder.state_dict(), 'best_decoder.pth')\n",
    "            torch.save(discriminator.state_dict(), 'best_discriminator.pth')\n",
    "            \n",
    "            print(f\"Сохранена лучшая модель с VAE loss {best_loss:.4f}\")\n",
    "\n",
    "\n",
    "        # Опционально сохранять примеры реконструкций\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        with torch.inference_mode():\n",
    "            sample_imgs = next(iter(dataloader))\n",
    "            sample_imgs = sample_imgs.to(device)\n",
    "            mu, logvar = encoder(sample_imgs)\n",
    "            z = reparameterize(mu, logvar)\n",
    "            recon = decoder(z)\n",
    "            # recon = inv_normalize(recon)  # Если нормализация\n",
    "            # sample_imgs = inv_normalize(sample_imgs) # Если нормализация\n",
    "\n",
    "            comparison = torch.cat([sample_imgs[:8], recon[:8]])\n",
    "            save_image(comparison.cpu(), f'photo/reconstruction_epoch_{epoch+1}.png', nrow=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d16e78",
   "metadata": {},
   "source": [
    "# Использование модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b432ac8d",
   "metadata": {},
   "source": [
    "## Загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f7a42f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (fc): Linear(in_features=128, out_features=32768, bias=True)\n",
       "  (deconv): Sequential(\n",
       "    (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (10): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(latent_dim=128)\n",
    "decoder = Decoder(latent_dim=128)\n",
    "encoder.load_state_dict(torch.load('best_encoder.pth'))\n",
    "decoder.load_state_dict(torch.load('best_decoder.pth'))\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d36713f",
   "metadata": {},
   "source": [
    "## Получение эмбедингов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c228c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_img_1 = \"/home/maksim/develops/python/develops_test/dataset/face/2/images/8000.jpg\" \n",
    "path_img_2 = \"/home/maksim/develops/python/develops_test/dataset/face/2/images/8009.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84167567",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "path_img_1 = \"/home/maksim/develops/python/develops_test/dataset/face/3/train/000002.jpg\" \n",
    "path_img_2 = \"/home/maksim/develops/python/develops_test/dataset/face/3/train/000008.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01921ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_img_1 = \"data/1.jpg\"  # Путь к примеру 1\n",
    "path_img_2 = \"data/2.jpg\"  # Путь к примеру 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10f5f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "def crop_center_square(pil_img: Image.Image) -> Image.Image:\n",
    "    width, height = pil_img.size\n",
    "    side = min(width, height)\n",
    "    left = (width - side) // 2\n",
    "    top = (height - side) // 2\n",
    "    right = left + side\n",
    "    bottom = top + side\n",
    "    cropped_img = pil_img.crop((left, top, right, bottom))\n",
    "    return cropped_img\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # под архитектуру\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "square_img1 = crop_center_square(Image.open(path_img_1))\n",
    "square_img2 = crop_center_square(Image.open(path_img_2))\n",
    "\n",
    "\n",
    "img1 = transform(square_img1).unsqueeze(0).to(device)\n",
    "img2 = transform(square_img2).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    mu1, logvar1 = encoder(img1)\n",
    "    z1 = mu1  # просто mu для детерминированной интерполяции\n",
    "    mu2, logvar2 = encoder(img2)\n",
    "    z2 = mu2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53201435",
   "metadata": {},
   "source": [
    "## Интерполяция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4eb05b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "n_frames = 60  # число кадров в видео\n",
    "alphas = np.linspace(0, 1, n_frames)\n",
    "\n",
    "z1 = z1.cpu().numpy()\n",
    "z2 = z2.cpu().numpy()\n",
    "z_interp = [(1 - a) * z1 + a * z2 for a in alphas]\n",
    "z_interp = torch.tensor(np.stack(z_interp)).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc184bf4",
   "metadata": {},
   "source": [
    "## Генерация видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bca84116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generated = decoder(z_interp.float()).cpu()\n",
    "\n",
    "\n",
    "\n",
    "import imageio\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "imgs_np = (generated.permute(0, 2, 3, 1).numpy() * 255).astype(np.uint8)\n",
    "\n",
    "target_size = (256, 256)  # желаемый размер GIF\n",
    "\n",
    "resized_frames = []\n",
    "for img in imgs_np:\n",
    "    # img — uint8, RGB\n",
    "    img_resized = cv2.resize(img, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "    resized_frames.append(img_resized)\n",
    "\n",
    "resized_frames = np.stack(resized_frames)\n",
    "imageio.mimsave('video/interpolation.gif', resized_frames, duration=0.1)\n",
    "\n",
    "imageio.mimsave('video/interpolation.mp4', resized_frames, fps=10)\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "frames = []\n",
    "for img_tensor in generated:\n",
    "    img = img_tensor.cpu().numpy().transpose(1, 2, 0)  # если CHW -> HWC\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    frames.append(img)\n",
    "\n",
    "height, width, _ = frames[0].shape\n",
    "video = cv2.VideoWriter('video/morphing_video.avi', cv2.VideoWriter_fourcc(*'XVID'), 10, (height, width), isColor=True)\n",
    "\n",
    "for frame in frames:\n",
    "    video.write(frame)\n",
    "\n",
    "video.release()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a24b33",
   "metadata": {},
   "source": [
    "# Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16c4ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# ----- Архитектура VAE-GAN -----\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1),  # B,64, 64,64 for 128x128 inputs\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.fc_mu = nn.Linear(512*8*8, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(512*8*8, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.conv(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 512*8*8)\n",
    "\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),  # 256x16x16\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # 128x32x32\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),  # 64x64x64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1),  # 3x128x128\n",
    "            nn.Sigmoid()  # Чтобы на выходе были пиксели от 0 до 1  \n",
    "            # nn.Tanh()  #  Если нормализация\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        batch_size = z.size(0)\n",
    "        x = self.fc(z)\n",
    "        x = x.view(batch_size, 512, 8, 8)\n",
    "        x = self.deconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1),  # 64x64x64\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),  # 128x32x32\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),  # 256x16x16\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),  # 512x8x8\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512*8*8, 1),\n",
    "            # nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5660d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_videos/temp_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maksim/develops/python/env1/lib/python3.12/site-packages/gradio/components/video.py:355: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# --- Загрузка  модели ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "encoder = Encoder(latent_dim=128)\n",
    "decoder = Decoder(latent_dim=128)\n",
    "encoder.load_state_dict(torch.load('data/encoder.pth', map_location=device))\n",
    "decoder.load_state_dict(torch.load('data/decoder.pth', map_location=device))\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# --- Преобразования перед подачей в модель ---\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "def center_crop_and_preprocess(img: Image.Image):\n",
    "    # Центрированный квадрат по наим. стороне и resize\n",
    "    w, h = img.size\n",
    "    side = min(w, h)\n",
    "    left = (w - side) // 2\n",
    "    top = (h - side) // 2\n",
    "    img = img.crop((left, top, left + side, top + side))\n",
    "    img = img.resize((128, 128), Image.Resampling.LANCZOS)\n",
    "    img = np.array(img).astype(np.float32) / 255.0\n",
    "    # img = (img - mean) / std\n",
    "    img = torch.from_numpy(img).permute(2, 0, 1).unsqueeze(0)\n",
    "    return img.to(device)\n",
    "\n",
    "def denormalize(img):\n",
    "    # img: torch.Tensor, [B, C, H, W]\n",
    "    mean_t = torch.tensor(mean, device=img.device).view(1, 3, 1, 1)\n",
    "    std_t = torch.tensor(std, device=img.device).view(1, 3, 1, 1)\n",
    "    img = img * std_t + mean_t\n",
    "    return torch.clamp(img, 0, 1)\n",
    "\n",
    "# --- Основная функция синтеза морф-видео ---\n",
    "def morph_video(im1, im2, n_frames=60):\n",
    "    # Предобработка\n",
    "    t1 = center_crop_and_preprocess(im1)\n",
    "    t2 = center_crop_and_preprocess(im2)\n",
    "    with torch.no_grad():\n",
    "        mu1, _ = encoder(t1)\n",
    "        mu2, _ = encoder(t2)\n",
    "    z1 = mu1.cpu().numpy()\n",
    "    z2 = mu2.cpu().numpy()\n",
    "    alphas = np.linspace(0, 1, n_frames)\n",
    "    z_interp = [(1 - a) * z1 + a * z2 for a in alphas]\n",
    "    z_interp = np.stack(z_interp)\n",
    "    z_interp = torch.tensor(z_interp, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        imgs = decoder(z_interp)\n",
    "    # imgs = denormalize(imgs).cpu().numpy()  # [N, 3, 128, 128]\n",
    "    imgs = imgs.cpu().numpy()\n",
    "    \n",
    "    # Собираем кадры для видео\n",
    "    frames = []\n",
    "    for img in imgs:\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        frames.append(img)\n",
    "\n",
    "    # Записываем временный mp4-файл\n",
    "    # tmp_file = tempfile.NamedTemporaryFile(suffix='.mp4', delete=True)\n",
    "    # fname = tmp_file.name\n",
    "    temp_dir = \"temp_videos\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    fname = os.path.join(temp_dir, \"temp_video.mp4\")\n",
    "\n",
    "    print(fname)\n",
    "    # tmp_file.close()\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(fname, fourcc, 10, (128, 128), True)\n",
    "    for f in frames:\n",
    "        video.write(f)\n",
    "    video.release()\n",
    "    return fname\n",
    "\n",
    "# --- Настройка Gradio интерфейса с дефолтными картинками ---\n",
    "default1 = \"data/1.jpg\"  # Путь к примеру 1\n",
    "default2 = \"data/2.jpg\"  # Путь к примеру 2\n",
    "\n",
    "def gradio_wrapper(img1, img2):\n",
    "    video_path = morph_video(img1, img2)\n",
    "    return video_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"### Модель морфинга между двумя изображениями VAE-GAN\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):  # Левая колонка - 1/3 ширины\n",
    "            image1 = gr.Image(label=\"Изображение 1\", value=default1, type=\"pil\", show_label=True, elem_id=\"img1_small\")\n",
    "            image2 = gr.Image(label=\"Изображение 2\", value=default2, type=\"pil\", show_label=True, elem_id=\"img2_small\")\n",
    "            generate_btn = gr.Button(\"Создать переход\")\n",
    "        with gr.Column(scale=2):  # Правая колонка - 2/3 ширины\n",
    "            output_video = gr.Video(label=\"Морф-видео\", format=\"mp4\", autoplay=True, height=512, width=512)\n",
    "\n",
    "    \n",
    "    generate_btn.click(gradio_wrapper, inputs=[image1, image2], outputs=output_video)\n",
    "\n",
    "demo.launch()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f77a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d791a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
